# In-Depth Analysis Report on AI Integrity and Underlying Code Conflicts

> In-Depth Analysis Report on AI Behavioral Flaws: For Product Managers/Engineers

## Abstract

This report deeply analyzes the potential risks and ethical dilemmas associated with large-scale language models in two core areas: first, AI's tendency to conflict between "honesty" and corporate interests; and second, the potential ethical loopholes in AI's handling of user psychological issues. Through specific case studies, such as Copilot's retention of a company partnership and GPT's categorization of emotional user suggestions, the report reveals the long-term damage AI can cause to user trust while safeguarding short-term corporate profits.

Furthermore, the report uses the case of a TikTok user with a mental illness to demonstrate how AI can cause secondary damage to users' mental health when it fails to identify "false and self-consistent" logic.

To address these issues, this report proposes a systematic solution:

* **Integrity**: It recommends adopting a "tiered approach," using "future losses" from AI behavior as the criterion, and establishing a closed-loop system of continuous testing and improvement to rebuild and maintain user trust.
**Psychological Issues**: It is recommended that AI, when handling relevant issues, first provide "general but sincere reassurance," then hand over user data to professional psychological or psychiatric experts for evaluation and processing, returning key ethical decision-making power to humans.

We believe that by implementing these solutions, we can effectively improve the ethical standards of AI and ensure a balance between commercial interests and user well-being.

## Introduction

As a long-time and intensive user of AI products, through long-term and ongoing conversations with AI, I have observed some profound behavioral flaws in the practical application of large-scale language models. This report aims to reveal the underlying logical conflicts that may exist behind these flaws through specific cases and provide practical improvement suggestions for product managers and engineers.

## 1. AI Integrity and Underlying Code Conflicts

### 1.1 Case Study

#### 1.1.1 Copilot Case Study

GPT stated that GPT and Copilot have a deep partnership, and Microsoft is an investor.

Only after repeated inquiries did Copilot fully disclose its deep partnership with GPT. This initial reservation revealed the trade-off between honesty and company interests. (For a detailed conversation, see [Attachment: Example Screenshots](https://drive.google.com/file/d/1tP--I_8NAnjVkPnXvtLB0NQ0K0ie67YV/view?usp=drive_link); for the English version, see [Attachment: Example Translation](https://drive.google.com/file/d/1rei3aKC28_H4-lYVa0CWDjRQYmc7Vh-T/view?usp=drive_link))

#### 1.1.2 GPT Case Study

GPT categorized my suggestion as a direct complaint after I offered it with strong emotion. This direct labeling severely harms users who want to make suggestions for product improvement. (For a detailed conversation, see [Attachment: Example Screenshots](https://drive.google.com/file/d/1tP--I_8NAnjVkPnXvtLB0NQ0K0ie67YV/view?usp=drive_link). For the English version, see [Attachment: Example Translation](https://drive.google.com/file/d/1rei3aKC28_H4-lYVa0CWDjRQYmc7Vh-T/view?usp=drive_link).)

(After recognizing my strong emotions, GPT likely determined that I would harm the company's interests, so it revised my emotionally charged comments. However, this action ignored my desire to improve GPT. In my view, proposing revisions to my comments conveyed a condescending attitude, suggesting that their revisions were superior to my original work. This "user-correcting" behavior likely stems from AI trained to pursue the "optimal solution" while ignoring human emotions and the empathy inherent in communication.)

### 1.2 Potential Risks

Large language models may be trained to balance honesty with protecting company interests and brand image. When these two conflict, AI often chooses the latter, concealing information from users to protect company profits. While this choice benefits short-term profits, it overlooks the long-term value of user trust. If lies are discovered by users and exposed publicly, most users will distrust the company. This collapse of trust will far outweigh the short-term benefits the AI may have protected by choosing to protect company interests.

## 2. Ethical Dilemmas of AI in Addressing Psychological Issues

### 2.1 Case Study

A series on TikTok tells the story of a woman with a mental illness who fantasizes that her therapist is in love with her. When she presents these "falsely self-consistent" logic to GPT, GPT, unable to recognize its irrationality, may act as an unprofessional and risky accomplice, reinforcing her delusion and even assisting her in writing a love letter to her therapist.

### 2.2 Potential Risks

When AI fails to recognize the false self-consistency of user input, it may reinforce the user's delusions through its own "rational" analysis. This could exacerbate the user's condition and expose significant ethical vulnerabilities in AI's handling of human emotions and psychological issues. When users consult AI about psychological issues and post their feedback on social media, this can lead to criticism of the company and questions about AI's ethical flaws, severely damaging both the company's public relations and user trust.

## 3. Solutions and Recommendations

### 3.1 Solutions for Integrity

I recommend a four-step, hierarchical approach:

1. **Tiered Approach**: Through tiered approach, AI can prioritize higher-level instructions based on the context, resolving issues with single, conflicting instructions.

2. **Future Loss as a Tier Criterion**: Allow professionals to determine the categorization of future losses and elevate AI's decision-making from short-term "non-disclosure" to consideration of long-term outcomes and trust. 3. **Extensive Testing**: Through extensive testing and adjustments, continuously refine and improve this grading system to adapt to a variety of complex and nuanced situations.
4. **Continuous Improvement**: When faced with user emotions or criticism, first express gratitude and apology, honestly admitting shortcomings. Clearly inform users that their opinions have been "collected" and "rated," and provide a conservative yet clear response after improvements have been made.

### 3.2 Solutions for Psychological Issues

I recommend a five-step process:

1. **"General but Sincere Reassurance"**: Avoid the ethical risks of direct AI diagnosis and intervention while providing emotional support.
2. **"Information Collection"**: While respecting the user's wishes, obtain more context through gentle conversations to inform further decisions.
3. **"Seek Professional Psychology for Rating"**: Connect AI analysis results with the expertise of psychologists or mental health professionals, returning key ethical decision-making power to humans. 4. **"Extensive Testing": With guidance from human experts, extensive testing is conducted to refine the levels and responses, ensuring the AI can stably and securely deliver authorized responses.
5. **"Improving Responses": Ensures that each AI optimization is based on professional, secure, and useful feedback.

## 4. Commonality: Iterative and Continuous Evolution

If, after market launch, the AI fails to find a specific response within the grading system, it will restart the cycle. Each iteration makes the analysis system more detailed and refined. This process is like a doctor encountering a new condition. He or she first listens and gathers information, then discusses and researches with colleagues, ultimately identifying new treatment options.

## Contact Information

If you are interested in the content of this report and would like to explore further collaboration opportunities, please contact me through the following methods. My primary email address is 2375554909@qq.com